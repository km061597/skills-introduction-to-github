input {
  # Receive logs from Filebeat
  beats {
    port => 5044
    host => "0.0.0.0"
  }

  # Alternative: Receive logs directly via TCP
  tcp {
    port => 5000
    codec => json
  }

  # Alternative: Receive logs via HTTP
  http {
    port => 8080
    codec => json
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "log"
    }
  }

  # Extract timestamp from log
  if [log][timestamp] {
    date {
      match => ["[log][timestamp]", "ISO8601"]
      target => "@timestamp"
    }
  }

  # Parse log level
  if [log][level] {
    mutate {
      add_field => { "log_level" => "%{[log][level]}" }
    }
  }

  # Extract service name
  if [log][service] {
    mutate {
      add_field => { "service" => "%{[log][service]}" }
    }
  } else if [container][name] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
    }
  }

  # Parse request ID for tracing
  if [log][request_id] {
    mutate {
      add_field => { "trace_id" => "%{[log][request_id]}" }
    }
  }

  # Extract HTTP details
  if [log][method] {
    mutate {
      add_field => {
        "http_method" => "%{[log][method]}"
        "http_path" => "%{[log][url]}"
        "http_status" => "%{[log][status_code]}"
      }
    }
  }

  # Parse duration for performance monitoring
  if [log][duration_ms] {
    mutate {
      convert => { "[log][duration_ms]" => "float" }
    }
  }

  # Add environment tag
  mutate {
    add_field => { "environment" => "${ENVIRONMENT:development}" }
  }

  # Parse stack traces for errors
  if [log][level] == "ERROR" or [log][level] == "CRITICAL" {
    if [log][exception] {
      mutate {
        add_field => { "has_exception" => "true" }
      }
    }
  }

  # Grok patterns for unstructured logs
  grok {
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}",
        "%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}"
      ]
    }
    overwrite => [ "message" ]
  }

  # Remove empty or null fields
  prune {
    blacklist_names => ["^$", "^null$"]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
    index => "smartamazon-logs-%{+YYYY.MM.dd}"

    # Use document ID to prevent duplicates
    document_id => "%{[@metadata][fingerprint]}"

    # Template for index
    template_name => "smartamazon-logs"
    template_overwrite => true
  }

  # Output to stdout for debugging (only in development)
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }

  # Send critical errors to separate index
  if [log_level] == "ERROR" or [log_level] == "CRITICAL" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
      index => "smartamazon-errors-%{+YYYY.MM.dd}"
    }
  }
}
